@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

@article{c9d4fbeac7324056bed5d1cb262a7268,
  title = "Character-level convolutional networks for text classification",
  abstract = "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
  author = "Xiang Zhang and Junbo Zhao and Yann Lecun",
  year = "2015",
  language = "English (US)",
  volume = "2015-January",
  pages = "649--657",
  journal = "Advances in Neural Information Processing Systems",
  issn = "1049-5258",
  note = "29th Annual Conference on Neural Information Processing Systems, NIPS 2015 ; Conference date: 07-12-2015 Through 12-12-2015",
}

@INPROCEEDINGS{8844895,
  author={A. C. {Pandey} and M. {Garg} and S. {Rajput}},
  booktitle={2019 Twelfth International Conference on Contemporary Computing (IC3)}, 
  title={Enhancing Text Mining Using Deep Learning Models}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/IC3.2019.8844895}
}

@inproceedings{iyyer-etal-2015-deep,
    title = "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    author = "Iyyer, Mohit  and
      Manjunatha, Varun  and
      Boyd-Graber, Jordan  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1162",
    doi = "10.3115/v1/P15-1162",
    pages = "1681--1691",
}

@inproceedings{johnson-zhang-2017-deep,
    title = "Deep Pyramid Convolutional Neural Networks for Text Categorization",
    author = "Johnson, Rie  and
      Zhang, Tong",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1052",
    doi = "10.18653/v1/P17-1052",
    pages = "562--570",
    abstract = "This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",
}

@article{DBLP:journals/corr/ConneauSBL16,
  author    = {Alexis Conneau and
               Holger Schwenk and
               Lo{\"{\i}}c Barrault and
               Yann LeCun},
  title     = {Very Deep Convolutional Networks for Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1606.01781},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01781},
  archivePrefix = {arXiv},
  eprint    = {1606.01781},
  timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ConneauSBL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/OSheaN15,
  author    = {Keiron O'Shea and
               Ryan Nash},
  title     = {An Introduction to Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1511.08458},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08458},
  archivePrefix = {arXiv},
  eprint    = {1511.08458},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OSheaN15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/AllahyariPASTGK17a,
  author    = {Mehdi Allahyari and
               Seyed Amin Pouriyeh and
               Mehdi Assefi and
               Saied Safaei and
               Elizabeth D. Trippe and
               Juan B. Gutierrez and
               Krys J. Kochut},
  title     = {A Brief Survey of Text Mining: Classification, Clustering and Extraction
               Techniques},
  journal   = {CoRR},
  volume    = {abs/1707.02919},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.02919},
  archivePrefix = {arXiv},
  eprint    = {1707.02919},
  timestamp = {Fri, 26 Mar 2021 11:10:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/AllahyariPASTGK17a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{porter1980algorithm,
  added-at = {2015-08-24T21:55:54.000+0200},
  author = {Porter, Martin F},
  biburl = {https://www.bibsonomy.org/bibtex/29ed77bc4c2a58578b5f6a7766064a352/thoni},
  interhash = {5cc5000d0cbdf957fd74a27ecbc3c2f7},
  intrahash = {9ed77bc4c2a58578b5f6a7766064a352},
  journal = {Program},
  keywords = {porter snowball stemming},
  number = 3,
  pages = {130--137},
  publisher = {MCB UP Ltd},
  timestamp = {2016-09-06T08:23:07.000+0200},
  title = {An algorithm for suffix stripping},
  volume = 14,
  year = 1980
}

@article{joulin2016fasttext,
  title={FastText.zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}

@article{c9d4fbeac7324056bed5d1cb262a7268,
  title = "Character-level convolutional networks for text classification",
  abstract = "This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.",
  author = "Xiang Zhang and Junbo Zhao and Yann Lecun",
  year = "2015",
  language = "English (US)",
  volume = "2015-January",
  pages = "649--657",
  journal = "Advances in Neural Information Processing Systems",
  issn = "1049-5258",
  note = "29th Annual Conference on Neural Information Processing Systems, NIPS 2015 ; Conference date: 07-12-2015 Through 12-12-2015",
}

@INPROCEEDINGS{8844895,
  author={A. C. {Pandey} and M. {Garg} and S. {Rajput}},
  booktitle={2019 Twelfth International Conference on Contemporary Computing (IC3)}, 
  title={Enhancing Text Mining Using Deep Learning Models}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/IC3.2019.8844895}
}

@inproceedings{iyyer-etal-2015-deep,
    title = "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    author = "Iyyer, Mohit  and
      Manjunatha, Varun  and
      Boyd-Graber, Jordan  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1162",
    doi = "10.3115/v1/P15-1162",
    pages = "1681--1691",
} 

@article{DBLP:journals/corr/OSheaN15,
  author    = {Keiron O'Shea and
               Ryan Nash},
  title     = {An Introduction to Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1511.08458},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08458},
  archivePrefix = {arXiv},
  eprint    = {1511.08458},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/OSheaN15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{LiuSentimentBook,
author = {Liu, Bing},
title = {Sentiment Analysis and Opinion Mining},
year = {2012},
isbn = {1608458849},
publisher = {Morgan and Claypool Publishers},
abstract = {Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.}
}

@article{Moraes2013DocumentlevelSC,
  title={Document-level sentiment classification: An empirical comparison between SVM and ANN},
  author={Rodrigo Moraes and Jo{\~a}o Francisco Valiati and Wilson P. Gavi{\~a}o Neto},
  journal={Expert Syst. Appl.},
  year={2013},
  volume={40},
  pages={621-633}
}

@misc{le2014distributed,
  title={Distributed Representations of Sentences and Documents}, 
  author={Quoc V. Le and Tomas Mikolov},
  year={2014},
  eprint={1405.4053},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{xavierDomain,
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  title = {Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach},
  year = {2011},
  isbn = {9781450306195},
  publisher = {Omnipress},
  address = {Madison, WI, USA},
  abstract = {The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.},
  booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
  pages = {513–520},
  numpages = {8},
  location = {Bellevue, Washington, USA},
  series = {ICML'11}
}

@inproceedings{zhaiEncoder,
  author = {Zhai, Shuangfei and Zhang, Zhongfei Mark},
  title = {Semisupervised Autoencoder for Sentiment Analysis},
  year = {2016},
  publisher = {AAAI Press},
  abstract = {In this paper, we investigate the usage of autoencoders in modeling textual data. Traditional autoencoders suffer from at least two aspects: scalability with the high dimensionality of vocabulary size and dealing with task-irrelevant words. We address this problem by introducing supervision via the loss function of autoencoders. In particular, we first train a linear classifier on the labeled data, then define a loss for the autoencoder with the weights learned from the linear classifier. To reduce the bias brought by one single classifier, we define a posterior probability distribution on the weights of the classifier, and derive the marginalized loss of the autoencoder with Laplace approximation. We show that our choice of loss function can be rationalized from the perspective of Bregman Divergence, which justifies the soundness of our model. We evaluate the effectiveness of our model on six sentiment analysis datasets, and show that our model significantly outperforms all the competing methods with respect to classification accuracy. We also show that our model is able to take advantage of unlabeled dataset and get improved performance. We further show that our model successfully learns highly discriminative feature maps, which explains its superior performance.},
  booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
  pages = {1394–1400},
  numpages = {7},
  location = {Phoenix, Arizona},
  series = {AAAI'16}
}

@inproceedings{johnson-zhang-2015-effective,
    title = "Effective Use of Word Order for Text Categorization with Convolutional Neural Networks",
    author = "Johnson, Rie  and
      Zhang, Tong",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N15-1011",
    doi = "10.3115/v1/N15-1011",
    pages = "103--112",
}

@inproceedings{tang-etal-2015-document,
    title = "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
    author = "Tang, Duyu  and
      Qin, Bing  and
      Liu, Ting",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1167",
    doi = "10.18653/v1/D15-1167",
    pages = "1422--1432",
}

@inproceedings{dou-2017-capturing,
    title = "Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network",
    author = "Dou, Zi-Yi",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1054",
    doi = "10.18653/v1/D17-1054",
    pages = "521--526",
    abstract = "Document-level sentiment classification is a fundamental problem which aims to predict a user{'}s overall sentiment about a product in a document. Several methods have been proposed to tackle the problem whereas most of them fail to consider the influence of users who express the sentiment and products which are evaluated. To address the issue, we propose a deep memory network for document-level sentiment classification which could capture the user and product information at the same time. To prove the effectiveness of our algorithm, we conduct experiments on IMDB and Yelp datasets and the results indicate that our model can achieve better performance than several existing methods.",
}

@misc{xu2016cached,
  title={Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification}, 
  author={Jiacheng Xu and Danlu Chen and Xipeng Qiu and Xuangjing Huang},
  year={2016},
  eprint={1610.04989},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{yang-etal-2016-hierarchical,
    title = "Hierarchical Attention Networks for Document Classification",
    author = "Yang, Zichao  and
      Yang, Diyi  and
      Dyer, Chris  and
      He, Xiaodong  and
      Smola, Alex  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1174",
    doi = "10.18653/v1/N16-1174",
    pages = "1480--1489",
}

@inproceedings{yin-etal-2017-document,
    title = "Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension",
    author = "Yin, Yichun  and
      Song, Yangqiu  and
      Zhang, Ming",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1217",
    doi = "10.18653/v1/D17-1217",
    pages = "2044--2054",
    abstract = "Document-level multi-aspect sentiment classification is an important task for customer relation management. In this paper, we model the task as a machine comprehension problem where pseudo question-answer pairs are constructed by a small number of aspect-related keywords and aspect ratings. A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions. We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism. Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines. We will release our code and data for the method replicability.",
}

@inproceedings{zhou-etal-2016-attention,
    title = "Attention-based {LSTM} Network for Cross-Lingual Sentiment Classification",
    author = "Zhou, Xinjie  and
      Wan, Xiaojun  and
      Xiao, Jianguo",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1024",
    doi = "10.18653/v1/D16-1024",
    pages = "247--256",
}

@inproceedings{ijcai2017-311,
  author    = {Zheng Li and Yu Zhang and Ying Wei and Yuxiang Wu and Qiang Yang},
  title     = {End-to-End Adversarial Memory Network for Cross-domain Sentiment Classification},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {2237--2243},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/311},
  url       = {https://doi.org/10.24963/ijcai.2017/311},
}

@inproceedings{socher-etal-2011-semi,
    title = "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
    author = "Socher, Richard  and
      Pennington, Jeffrey  and
      Huang, Eric H.  and
      Ng, Andrew Y.  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D11-1014",
    pages = "151--161",
}

@inproceedings{socher-etal-2012-semantic,
    title = "Semantic Compositionality through Recursive Matrix-Vector Spaces",
    author = "Socher, Richard  and
      Huval, Brody  and
      Manning, Christopher D.  and
      Ng, Andrew Y.",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D12-1110",
    pages = "1201--1211",
}

@inproceedings{socher-etal-2013-recursive,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@inproceedings{qian-etal-2015-learning,
    title = "Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network",
    author = "Qian, Qiao  and
      Tian, Bo  and
      Huang, Minlie  and
      Liu, Yang  and
      Zhu, Xuan  and
      Zhu, Xiaoyan",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1132",
    doi = "10.3115/v1/P15-1132",
    pages = "1365--1374",
}

@inproceedings{kalchbrenner-etal-2014-convolutional,
    title = "A Convolutional Neural Network for Modelling Sentences",
    author = "Kalchbrenner, Nal  and
      Grefenstette, Edward  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-1062",
    doi = "10.3115/v1/P14-1062",
    pages = "655--665",
}

@inproceedings{kim-2014-convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{wang-etal-2015-predicting,
    title = "Predicting Polarities of Tweets by Composing Word Embeddings with Long Short-Term Memory",
    author = "Wang, Xin  and
      Liu, Yuanchao  and
      Sun, Chengjie  and
      Wang, Baoxun  and
      Wang, Xiaolong",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1130",
    doi = "10.3115/v1/P15-1130",
    pages = "1343--1353",
}

@inproceedings{wang-etal-2016-dimensional,
    title = "Dimensional Sentiment Analysis Using a Regional {CNN}-{LSTM} Model",
    author = "Wang, Jin  and
      Yu, Liang-Chih  and
      Lai, K. Robert  and
      Zhang, Xuejie",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2037",
    doi = "10.18653/v1/P16-2037",
    pages = "225--230",
}

@inproceedings{wang-etal-2016-combination,
    title = "Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts",
    author = "Wang, Xingyou  and
      Jiang, Weijie  and
      Luo, Zhiyong",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1229",
    pages = "2428--2437",
    abstract = "Sentiment analysis of short texts is challenging because of the limited contextual information they usually contain. In recent years, deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been applied to text sentiment analysis with comparatively remarkable results. In this paper, we describe a jointed CNN and RNN architecture, taking advantage of the coarse-grained local features generated by CNN and long-distance dependencies learned via RNN for sentiment analysis of short texts. Experimental results show an obvious improvement upon the state-of-the-art on three benchmark corpora, MR, SST1 and SST2, with 82.28{\%}, 51.50{\%} and 89.95{\%} accuracy, respectively.",
}

@inproceedings{guggilla-etal-2016-cnn,
    title = "{CNN}- and {LSTM}-based Claim Classification in Online User Comments",
    author = "Guggilla, Chinnappa  and
      Miller, Tristan  and
      Gurevych, Iryna",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1258",
    pages = "2740--2751",
    abstract = "When processing arguments in online user interactive discourse, it is often necessary to determine their bases of support. In this paper, we describe a supervised approach, based on deep neural networks, for classifying the claims made in online arguments. We conduct experiments using convolutional neural networks (CNNs) and long short-term memory networks (LSTMs) on two claim data sets compiled from online user comments. Using different types of distributional word embeddings, but without incorporating any rich, expensive set of features, we achieve a significant improvement over the state of the art for one data set (which categorizes arguments as factual vs. emotional), and performance comparable to the state of the art on the other data set (which categorizes propositions according to their verifiability). Our approach has the advantages of using a generalized, simple, and effective methodology that works for claim categorization on different data sets and tasks.",
}

@article{huaEncode,
  author = {Huang, Minlie and Qian, Qiao and Zhu, Xiaoyan},
  title = {Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification},
  year = {2017},
  issue_date = {June 2017},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {35},
  number = {3},
  issn = {1046-8188},
  url = {https://doi.org/10.1145/3052770},
  doi = {10.1145/3052770},
  abstract = {Phrase/Sentence representation is one of the most important problems in natural language processing. Many neural network models such as Convolutional Neural Network (CNN), Recursive Neural Network (RNN), and Long Short-Term Memory (LSTM) have been proposed to learn representations of phrase/sentence, however, rich syntactic knowledge has not been fully explored when composing a longer text from its shorter constituent words. In most traditional models, only word embeddings are utilized to compose phrase/sentence representations, while the syntactic information of words is yet to be explored. In this article, we discover that encoding syntactic knowledge (part-of-speech tag) in neural networks can enhance sentence/phrase representation. Specifically, we propose to learn tag-specific composition functions and tag embeddings in recursive neural networks, and propose to utilize POS tags to control the gates of tree-structured LSTM networks. We evaluate these models on two benchmark datasets for sentiment classification, and demonstrate that improvements can be obtained with such syntactic knowledge encoded.},
  journal = {ACM Trans. Inf. Syst.},
  month = jun,
  articleno = {26},
  numpages = {27},
  keywords = {sentiment analysis, deep learning, Neural networks, recursive neural network, sentiment classification, representation learning, long short-term memory}
}

@inproceedings{guaWeak,
  author = {Guan, Ziyu and Chen, Long and Zhao, Wei and Zheng, Yi and Tan, Shulong and Cai, Deng},
  title = {Weakly-Supervised Deep Learning for Customer Review Sentiment Classification},
  year = {2016},
  isbn = {9781577357704},
  publisher = {AAAI Press},
  abstract = {Sentiment analysis is one of the key challenges for mining online user generated content. In this work, we focus on customer reviews which are an important form of opinionated content. The goal is to identify each sentence's semantic orientation (e.g. positive or negative) of a review. Traditional sentiment classification methods often involve substantial human efforts, e.g. lexicon construction, feature engineering. In recent years, deep learning has emerged as an effective means for solving sentiment classification problems. A neural network intrinsically learns a useful representation automatically without human efforts. However, the success of deep learning highly relies on the availability of large-scale training data. In this paper, we propose a novel deep learning framework for review sentiment classification which employs prevalently available ratings as weak supervision signals. The framework consists of two steps: (1) learn a high level representation (embedding space) which captures the general sentiment distribution of sentences through rating information; (2) add a classification layer on top of the embedding layer and use labeled sentences for supervised fine-tuning. Experiments on review data obtained from Amazon show the efficacy of our method and its superiority over baseline methods.},
  booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
  pages = {3719–3725},
  numpages = {7},
  location = {New York, New York, USA},
  series = {IJCAI'16}
}

@inproceedings{teng-etal-2016-context,
    title = "Context-Sensitive Lexicon Features for Neural Sentiment Analysis",
    author = "Teng, Zhiyang  and
      Vo, Duy-Tin  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1169",
    doi = "10.18653/v1/D16-1169",
    pages = "1629--1638",
}

@inproceedings{yu-jiang-2016-learning,
    title = "Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification",
    author = "Yu, Jianfei  and
      Jiang, Jing",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1023",
    doi = "10.18653/v1/D16-1023",
    pages = "236--246",
}

@inproceedings{ijcai2017-494,
  author    = {Zhou Zhao and Hanqing Lu and Deng Cai and Xiaofei He and Yueting Zhuang},
  title     = {Microblog Sentiment Classiﬁcation via Recurrent Random Walk Network Learning},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {3532--3538},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/494},
  url       = {https://doi.org/10.24963/ijcai.2017/494},
}

@inproceedings{mishra-etal-2017-learning,
    title = "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
    author = "Mishra, Abhijit  and
      Dey, Kuntal  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1035",
    doi = "10.18653/v1/P17-1035",
    pages = "377--387",
    abstract = "Cognitive NLP systems- i.e., NLP systems that make use of behavioral data - augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc. Such extraction of features is typically manual. We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like Sentiment Analysis and Sarcasm Detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement/gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
}

@inproceedings{qian-etal-2017-linguistically,
    title = "Linguistically Regularized {LSTM} for Sentiment Classification",
    author = "Qian, Qiao  and
      Huang, Minlie  and
      Lei, Jinhao  and
      Zhu, Xiaoyan",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1154",
    doi = "10.18653/v1/P17-1154",
    pages = "1679--1689",
    abstract = "This paper deals with sentence-level sentiment classification. Though a variety of neural network models have been proposed recently, however, previous models either depend on expensive phrase-level annotation, most of which has remarkably degraded performance when trained with only sentence-level annotation; or do not fully employ linguistic resources (e.g., sentiment lexicons, negation words, intensity words). In this paper, we propose simple models trained with sentence-level annotation, but also attempt to model the linguistic role of sentiment lexicons, negation words, and intensity words. Results show that our models are able to capture the linguistic role of sentiment words, negation words, and intensity words in sentiment expression.",
}

@inproceedings{68dong-etal-2014-adaptive,
    title = "Adaptive Recursive Neural Network for Target-dependent {T}witter Sentiment Classification",
    author = "Dong, Li  and
      Wei, Furu  and
      Tan, Chuanqi  and
      Tang, Duyu  and
      Zhou, Ming  and
      Xu, Ke",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-2009",
    doi = "10.3115/v1/P14-2009",
    pages = "49--54",
}

@inproceedings{70tang-etal-2016-effective,
    title = "Effective {LSTM}s for Target-Dependent Sentiment Classification",
    author = "Tang, Duyu  and
      Qin, Bing  and
      Feng, Xiaocheng  and
      Liu, Ting",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1311",
    pages = "3298--3307",
    abstract = "Target-dependent sentiment classification remains a challenge: modeling the semantic relatedness of a target with its context words in a sentence. Different context words have different influences on determining the sentiment polarity of a sentence towards the target. Therefore, it is desirable to integrate the connections between target word and context words when building a learning system. In this paper, we develop two target dependent long short-term memory (LSTM) models, where target information is automatically taken into account. We evaluate our methods on a benchmark dataset from Twitter. Empirical results show that modeling sentence representation with standard LSTM does not perform well. Incorporating target information into LSTM can significantly boost the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.",
}

@inproceedings{71ruder-etal-2016-hierarchical,
    title = "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis",
    author = "Ruder, Sebastian  and
      Ghaffari, Parsa  and
      Breslin, John G.",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1103",
    doi = "10.18653/v1/D16-1103",
    pages = "999--1005",
}

@article{72Zhang_Zhang_Vo_2016, 
  title={Gated Neural Networks for Targeted Sentiment Analysis}, 
  volume={30}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10380}, 
  abstractNote={ &lt;p&gt; Targeted sentiment analysis classifies the sentiment polarity towards each target entity mention in given text documents. Seminal methods extract manual discrete features from automatic syntactic parse trees in order to capture semantic information of the enclosing sentence with respect to a target entity mention. Recently, it has been shown that competitive accuracies can be achieved without using syntactic parsers, which can be highly inaccurate on noisy text such as tweets. This is achieved by applying distributed word representations and rich neural pooling functions over a simple and intuitive segmentation of tweets according to target entity mentions. In this paper, we extend this idea by proposing a sentence-level neural model to address the limitation of pooling functions, which do not explicitly model tweet-level semantics. First, a bi-directional gated neural network is used to connect the words in a tweet so that pooling functions can be applied over the hidden layer instead of words for better representing the target and its contexts. Second, a three-way gated neural network structure is used to model the interaction between the target mention and its surrounding contexts. Experiments show that our proposed model gives significantly higher accuracies compared to the current best method for targeted sentiment analysis. &lt;/p&gt; }, 
  number={1}, 
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Zhang, Meishan and Zhang, Yue and Vo, Duy-Tin},
  year={2016}, 
  month={Mar.} 
}

@inproceedings{73wang-etal-2016-attention,
    title = "Attention-based {LSTM} for Aspect-level Sentiment Classification",
    author = "Wang, Yequan  and
      Huang, Minlie  and
      Zhu, Xiaoyan  and
      Zhao, Li",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1058",
    doi = "10.18653/v1/D16-1058",
    pages = "606--615",
}

@inproceedings{74YANGATT,
  author = {Yang, Min and Tu, Wenting and Wang, Jingxuan and Xu, Fei and Chen, Xiaojun},
  title = {Attention-Based LSTM for Target-Dependent Sentiment Classification},
  year = {2017},
  publisher = {AAAI Press},
  abstract = {We present an attention-based bidirectional LSTM approach to improve the target-dependent sentiment classification. Our method learns the alignment between the target entities and the most distinguishing features. We conduct extensive experiments on a real-life dataset. The experimental results show that our model achieves state-of-the-art results.},
  booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
  pages = {5013–5014},
  numpages = {2},
  location = {San Francisco, California, USA},
  series = {AAAI'17}
}

@inproceedings{75liu-zhang-2017-attention,
    title = "Attention Modeling for Targeted Sentiment",
    author = "Liu, Jiangming  and
      Zhang, Yue",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2091",
    pages = "572--577",
    abstract = "Neural network models have been used for target-dependent sentiment analysis. Previous work focus on learning a target specific representation for a given input sentence which is used for classification. However, they do not explicitly model the contribution of each word in a sentence with respect to targeted sentiment polarities. We investigate an attention model to this end. In particular, a vanilla LSTM model is used to induce an attention value of the whole sentence. The model is further extended to differentiate left and right contexts given a certain target following previous work. Results show that by using attention to model the contribution of each word with respect to the target, our model gives significantly improved results over two standard benchmarks. We report the best accuracy for this task.",
}

@misc{76tang2016aspect,
      title={Aspect Level Sentiment Classification with Deep Memory Network}, 
      author={Duyu Tang and Bing Qin and Ting Liu},
      year={2016},
      eprint={1605.08900},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{78Li_2017,
   title={Deep Memory Networks for Attitude Identification},
   ISBN={9781450346757},
   url={http://dx.doi.org/10.1145/3018661.3018714},
   DOI={10.1145/3018661.3018714},
   journal={Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
   publisher={ACM},
   author={Li, Cheng and Guo, Xiaoxiao and Mei, Qiaozhu},
   year={2017},
   month={Feb}
}

@misc{79ma2017interactive,
      title={Interactive Attention Networks for Aspect-Level Sentiment Classification}, 
      author={Dehong Ma and Sujian Li and Xiaodong Zhang and Houfeng Wang},
      year={2017},
      eprint={1709.00893},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{80chen-etal-2017-recurrent,
    title = "Recurrent Attention Network on Memory for Aspect Sentiment Analysis",
    author = "Chen, Peng  and
      Sun, Zhongqian  and
      Bing, Lidong  and
      Yang, Wei",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1047",
    doi = "10.18653/v1/D17-1047",
    pages = "452--461",
    abstract = "We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.",
}

@inproceedings{81TayDy,
  author = {Tay, Yi and Tuan, Luu Anh and Hui, Siu Cheung},
  title = {Dyadic Memory Networks for Aspect-Based Sentiment Analysis},
  year = {2017},
  isbn = {9781450349185},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3132847.3132936},
  doi = {10.1145/3132847.3132936},
  abstract = {This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions or holographic compositions into the memory selection operation. To this end, we propose two variations of our dyadic memory networks, namely the Tensor DyMemNN and Holo DyMemNN. Overall, our two models are end-to-end neural architectures that enable rich dyadic interaction between aspect and document which intuitively leads to better performance. Via extensive experiments, we show that our proposed models achieve the state-of-the-art performance and outperform many neural architectures across six benchmark datasets.},
  booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
  pages = {107–116},
  numpages = {10},
  keywords = {aspect, deep learning, aspect-based sentiment analysis, neural networks, sentiment analysis, aspect-level sentiment analysis},
  location = {Singapore, Singapore},
  series = {CIKM '17}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@article{lin2021survey,
  title={A Survey of Transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2106.04554},
  year={2021}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{schmidt2020data,
  title={Data mining in clinical trial text: Transformers for classification and question answering tasks},
  author={Schmidt, Lena and Weeds, Julie and Higgins, Julian},
  journal={arXiv preprint arXiv:2001.11268},
  year={2020}
}

@article{zhang2018deep,
  title={Deep learning for sentiment analysis: A survey},
  author={Zhang, Lei and Wang, Shuai and Liu, Bing},
  journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  volume={8},
  number={4},
  pages={e1253},
  year={2018},
  publisher={Wiley Online Library}
}

@article{dos2014deep,
  title={Deep convolutional neural networks for sentiment analysis of short texts},
  author={Dos Santos, Cicero and Gatti, Maira},
  booktitle={Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  pages={69--78},
  year={2014}
}

@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{akhtar2017multilayer,
  title={A multilayer perceptron based ensemble technique for fine-grained financial sentiment analysis},
  author={Akhtar, Md Shad and Kumar, Abhishek and Ghosal, Deepanway and Ekbal, Asif and Bhattacharyya, Pushpak},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={540--546},
  year={2017}
}

@article{conneau2016very,
  title={Very deep convolutional networks for text classification},
  author={Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"\i}c and Lecun, Yann},
  journal={arXiv preprint arXiv:1606.01781},
  year={2016}
}

@article{liu2016recurrent,
  title={Recurrent neural network for text classification with multi-task learning},
  author={Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:1605.05101},
  year={2016}
}

@article{dieng2016topicrnn,
  title={Topicrnn: A recurrent neural network with long-range semantic dependency},
  author={Dieng, Adji B and Wang, Chong and Gao, Jianfeng and Paisley, John},
  journal={arXiv preprint arXiv:1611.01702},
  year={2016}
}

@misc{miyato2017adversarial,
      title={Adversarial Training Methods for Semi-Supervised Text Classification}, 
      author={Takeru Miyato and Andrew M. Dai and Ian Goodfellow},
      year={2017},
      eprint={1605.07725},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{yang2020survey,
      title={A Survey of Deep Learning Techniques for Neural Machine Translation}, 
      author={Shuoheng Yang and Yuxin Wang and Xiaowen Chu},
      year={2020},
      eprint={2002.07526},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sutskever2014sequence,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2016deep,
      title={Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation}, 
      author={Jie Zhou and Ying Cao and Xuguang Wang and Peng Li and Wei Xu},
      year={2016},
      eprint={1606.04199},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bapna2018training,
      title={Training Deeper Neural Machine Translation Models with Transparent Attention}, 
      author={Ankur Bapna and Mia Xu Chen and Orhan Firat and Yuan Cao and Yonghui Wu},
      year={2018},
      eprint={1808.07561},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2019learning,
      title={Learning Deep Transformer Models for Machine Translation}, 
      author={Qiang Wang and Bei Li and Tong Xiao and Jingbo Zhu and Changliang Li and Derek F. Wong and Lidia S. Chao},
      year={2019},
      eprint={1906.01787},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and Blunsom, Phil},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1700--1709},
  year={2013}
}

@misc{gehring2017convolutional,
      title={A Convolutional Encoder Model for Neural Machine Translation}, 
      author={Jonas Gehring and Michael Auli and David Grangier and Yann N. Dauphin},
      year={2017},
      eprint={1611.02344},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dai2019transformerxl,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jozefowicz2016exploring,
      title={Exploring the Limits of Language Modeling}, 
      author={Rafal Jozefowicz and Oriol Vinyals and Mike Schuster and Noam Shazeer and Yonghui Wu},
      year={2016},
      eprint={1602.02410},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lample2016neural,
      title={Neural Architectures for Named Entity Recognition}, 
      author={Guillaume Lample and Miguel Ballesteros and Sandeep Subramanian and Kazuya Kawakami and Chris Dyer},
      year={2016},
      eprint={1603.01360},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ma2016endtoend,
      title={End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF}, 
      author={Xuezhe Ma and Eduard Hovy},
      year={2016},
      eprint={1603.01354},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yang2020xlnet,
      title={XLNet: Generalized Autoregressive Pretraining for Language Understanding}, 
      author={Zhilin Yang and Zihang Dai and Yiming Yang and Jaime Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
      year={2020},
      eprint={1906.08237},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{denk2019bertgrid,
      title={BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding}, 
      author={Timo I. Denk and Christian Reisswig},
      year={2019},
      eprint={1909.04948},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{katti2018chargrid,
      title={Chargrid: Towards Understanding 2D Documents}, 
      author={Anoop Raveendra Katti and Christian Reisswig and Cordula Guder and Sebastian Brarda and Steffen Bickel and Johannes Höhne and Jean Baptiste Faddoul},
      year={2018},
      eprint={1809.08799},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}